---
title: "Tutorial - Insurance Application"
author: "Valentina Ghidini"
date: "14/6/2022"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, echo = F}
install.packages("invgamma")
install.packages("truncnorm")
install.packages("bnpResiduals")
install.packages("ggplot2")
install.packages("mcclust.ext")

```

```{r}
#library(orthopolynom)
#library(MASS)
library(invgamma)
library(truncnorm)
library(bnpResiduals)
library(ggplot2)
library(mcclust.ext)
library(bnpResiduals)
```

# The data

The dataset has been taken from \texttt{https://github.com/stedy/Machine-Learning-with-R-datasets}. The target variable of interest is provided by the (log) medical cost for each individual, that we want to predict with respect to different covariates of interest (such as BMI, age, smoker, number of children, sex, region of residence - northeast, northwest, southeast, southwest).

```{r}
insurance <- read.csv("insurance.csv")
```

Let's divide the data so that we have distint groups for different regions. and let's relevel the factor.

```{r}
groups <- factor(insurance$region)
levels(groups) <- c(1,2,3,4)
insurance <- insurance[,-6]
insurance$sex <- ifelse(insurance$sex == 'female', 1, 0) # 1 female, 0 male
insurance$smoker <- ifelse(insurance$smoker == 'yes', 1, 0)
```

Finally, let's set up the design matrix X and the target Y.

```{r}
Y <- log(insurance$charges)
X <- as.matrix(insurance[,1:5])
X <- cbind(rep(1, nrow(X)), X) # add the intercept
```


# Initialization and hyperparameters for the Gibbs sampler model

Now, let's set all the hyperparameters we need to run the model.

```{r}
n <- nrow(X) # number of observations
k <- ncol(X)-1 # number of variables
```


```{r}
b0 <-  rep(0, k+1) # mean of the coefficients

B0 <-  diag(10, k+1) # covariance matrix of the coefficients

s1 <- s2 <-  2
S1 <- S2 <- 1
sigma2=10 # variance
theta = 1
```

Let's initialize the parameters and the residuals with the OLS estimates, and the component (the $\gamma_i$) randomly:
```{r}
ols_model <- lm(Y~X[,-1])
b <- coef(ols_model)
epsilon <- Y-X%*%b # ols residuals
comp <- ifelse(epsilon > 0, 1, -1)
```

Let's set the number of components of the Blocked Gibbs sampler:

```{r}
N = 150 # number of components in the blocked Gibbs sampler
log_s = T # compute all the probabilities in log-scale
```

Finally, let's initialize $\mu_0, p, \sigma_0$:

```{r}
p <- rep(1/N, N)
mu0 <- 1
sigma0 <- sqrt(25)
```

and let's create the object initializer, needed as input for the models:

```{r}
# simulate the number of components of the RPM
K <- sample(1:N, size = n, replace = T, prob = p)
# Simulate the triplets
Z <- cbind(rtruncnorm(N,a = 0, mean = mu0, sd = sigma0), sqrt(rinvgamma(N,shape = s1, rate = S1)), sqrt(rinvgamma(N,shape = s2, rate = S2)))

temp <- bnpResiduals:::sampler_triplet_blockedGS(Y,epsilon, comp, p, Z, mu0, sigma0)
K <- temp$K
Z <- temp$Z
complete_matrix = Z[K,]

# Initializer object
initializer_ <- bnpResiduals:::initializer(Z = Z, p = p, comp = comp, b0 = b0, B0 = B0, 
        N = N, mu0 = mu0, sigma0 = sigma0, complete_matrix = complete_matrix)

```


# Model and Algorithm

```{r}
ITERS <- 1000
BURNIN <- 100
THIN <- 0
```

The next cell estimate the model. Be patient, it takes a few minutes! :) Also, mind that some CSV (on per parameter) will be created in the working directory. They will be needed for the diagnostic part.

```{r}
linearModel <- bnp.lm(X, Y, initializer_, cluster=F, iter=ITERS, burn_in = BURNIN, thin = THIN, sigma = 0)
```

Let's read the parameters:

```{r}
eps <- read.table("epsilon.csv", header=F, skip=1, sep=";")
tau1 <-  read.table("tau1.csv", header=F, skip=1, sep=";")
tau2 <-  read.table("tau2.csv", header=F, skip=1, sep=";")
mu <-  read.table("mu.csv", header=F, skip=1, sep=";")
component <-  read.table("component.csv", header=F, skip=1, sep=";")
weights <-  read.table("weights.csv", header=F, skip=1, sep=";")
m <-read.table("betas.csv", header=F, skip=1, sep=";")

```



Let's check the convergence for a coefficient of interest:

```{r}
coef_of_interest <- 2
beta_est <- m[,coef_of_interest]

par(mfrow=c(1,2))
# Autocorrelation
acf(beta_est, ylab="Sampled Value", main="ACF")
# Traceplots
plot(beta_est, type="l", ylab="Sampled Value", xlab="Iterations", main="Traceplot")
abline(h=mean(beta_est, na.rm=T), col=3, lty=2)

```


# Residual Densities - Empirical and Theoretical

Let's see the predictions, and the residual density (both theoretical and empirical):

```{r}
predictions <- X%*%linearModel$coef[,1]
residuals <- data.frame(res = Y - predictions)
```


```{r}
# Empirical Residual Density
ggplot(residuals, aes(x=res)) +
  geom_density(alpha=.8, color="blue") +
  ggtitle("Residual densities (empirical)") + xlab("Residual") + ylab("Density")


```

# Analysis for different groups + clusters

Here we need to write things more explicitly. This may be useful if you are not satisfied with the output of the \texttt{bnp.lm()} function, but you want more hands-on (and personalizable) results.

About the clusters: they can also be obtained from bnp.lm() simply setting \texttt{cluster = T}.

We also put an hyperprior on $\beta_0$ and $\sigma^2$
```{r}
sample_posterior_beta0 <- function(beta, b0, B, B0){
  if (is.null(dim(b0))){b0 <- matrix(b0, ncol=1)}
  if (is.null(dim(beta))){beta <- matrix(beta, ncol=1)}
  mean_vector <- apply(beta, 1, mean)
  L <- ncol(beta)
  k <- length(beta)
  new_mean <- solve(L*solve(B) + solve(B0))%*%(L*solve(B)%*%mean_vector+solve(B0)%*%b0)
  new_var <- solve(solve(B) + solve(B0))
  beta0_post <- mvrnorm(1, mu= new_mean, Sigma = new_var)
  return(beta0_post)
}

posterior_sigma2 <- function(c0, d0, X, Y, beta, groups){
  N <- nrow(X)
  new_shape <- c0 + N/2
  residuals <- rep(0, N)
  L <- length(unique(groups))
  for (l in 1:L){
    residuals[groups==l] <- Y[groups==l] - X[groups==l,] %*% beta[,l]
  }
  new_scale <- d0 + .5*sum(residuals^2)
  sigma2_post <- rinvgamma(1, shape=new_shape, rate = new_scale)
  return(sigma2_post)
}


```


```{r}
GS_nonparametric <- function(X, Y, groups, initializer, iter, Verbose=T, log_rate=T, sigma=0, c0=1, d0=1){
  n <- nrow(X)
  k <- ncol(X)
  # Write on file
  null_df <- data.frame(NULL)
  Grid <- seq(-50, 50, by=.1)
  predictive <- matrix(0, ncol=length(Grid), nrow=iter)
  write.table(null_df, file = "mu.csv", row.names = F)
  write.table(null_df, file = "tau1.csv", row.names = F)
  write.table(null_df, file = "tau2.csv", row.names = F)
  write.table(null_df, file = "weights.csv", row.names = F)
  write.table(null_df, file = "betas.csv", row.names = F)
  write.table(null_df, file = "component.csv", row.names = F)
  write.table(null_df, file = "epsilon.csv", row.names = F)

  # Hyperpriors
  t=2
  T_big=4

  mu0=0 # To ensure semiconjugacy
  sigma0=initializer$sigma0

  pb <- progress_bar$new(total = iter)
  Z <- initializer$Z
  b0 <- initializer$b0
  B0 <- initializer$B0
  comp <- initializer$comp
  p <- initializer$p

  sigma0_saved <- c()
  complete_matrix = initializer$complete_matrix

  # Cluster analysis for outliers
  clusters <- matrix(0, nrow = iter, ncol=n)

  B <- matrix(0, ncol=ncol(X), nrow=ncol(X))
  diag(B) <- 1
  beta_post_full <- list()
  ZZ <- list()
  sigma2_post <- c()
  p_weights <- matrix(0, nrow=N, ncol=iter)
  for (j in 1:iter){
    if (Verbose){
      pb$tick()
      Sys.sleep(1 /iter)}

    L <- length(unique(groups))

    beta_post <- matrix(0, ncol=L, nrow = ncol(X))
    beta0_post <- matrix(0, ncol=iter, nrow = p)
    # sample betas for each group

    for (l in 1:L){
      
      ss <- bnpResiduals:::sampler_beta(complete_matrix[groups==l,1], complete_matrix[groups==l,2], complete_matrix[groups==l,3], comp[groups==l], Y[groups==l], X[groups==l,], B0, matrix(b0, ncol=1))
      beta_post[,l] <- as.numeric(ss$beta)
      
      # Update Residuals
      epsilon[groups==l]<-Y[groups==l]-X[groups==l,]%*%beta_post[,l]
      
    }
    beta0 <- sample_posterior_beta0(beta_post, b0, B, B0)
    sigma2 <- posterior_sigma2(c0, d0, X, Y, beta_post, groups)

    beta0_post[,j] <- beta0
    beta_post_full[[j]] <- beta_post
    sigma2_post <- c(sigma2_post, sigma2)
    # Sample beta - for each group
   
    # Gamma prob
    w <- bnpResiduals:::sampler_gamma_prob(epsilon, complete_matrix[,1], complete_matrix[,2], complete_matrix[,3], log_rate = log_rate)

    #Update Gamma (comp)
    comp = apply(w,1,function(x){
      return(sample(c(1,-1), size=1, prob = x))
    })

    # Sample mu
    gs <- bnpResiduals:::sampler_triplet_blockedGS(Y, epsilon, comp, p, Z, mu0, sigma0)

    # Labels - they identify the cluster of belonging for each observation
    K <- gs$K
    clusters[j,] <- K
    Z <- gs$Z
    ZZ[[j]] <- Z
    complete_matrix <- gs$complete_matrix

    # Update p - stick breaking probabilities
    p <- bnpResiduals:::p_sampler(K, theta, N, sigma)

    p_weights[,j] <- p

    K_unique <- unique(K)

    # Update sigma0 - thanks to semiconjugacy
    sigma0 = sqrt(rinvgamma(n=1, t + length(K_unique)/2, T_big+0.5*sum(Z[K_unique,1]^2)))


    # Compute empirical predictive distribution on a grid of points
    predictive[j,] = sapply(Grid, function(x) sum(p*(.5*dnorm(x, Z[,1], sd=Z[,2])+.5*dnorm(x, -Z[,1], sd=Z[,3]))))
    ll <- list(beta = beta_post_full, component = comp, beta0 = beta0_post,
               weights = w, mu = complete_matrix[,1], tau1 = complete_matrix[,2], tau2=complete_matrix[,3], epsilon = epsilon)


    # Append to file
   write.table(matrix(ll$component, nrow=1), "component.csv", sep = ";", col.names = !file.exists("component.csv"), append = T, row.names = F)
    write.table(matrix(ll$weights, nrow=1), "weights.csv", sep = ";", col.names = !file.exists("weights.csv"), append = T, row.names = F)
    write.table(matrix(ll$mu, nrow=1), "mu.csv", sep = ";", col.names = !file.exists("mu.csv"), append = T, row.names = F)
    write.table(matrix(ll$tau1, nrow=1), "tau1.csv", sep = ";", col.names = !file.exists("tau1.csv"), append = T, row.names = F)
    write.table(matrix(ll$tau2, nrow=1), "tau2.csv", sep = ";", col.names = !file.exists("tau2.csv"), append = T, row.names = F)
    write.table(matrix(ll$epsilon, nrow=1), "epsilon.csv", sep = ";", col.names = !file.exists("epsilon.csv"), append = T, row.names = F)
    write.table(matrix(ll$beta0, nrow=1), "beta0.csv", sep = ";", col.names = !file.exists("beta0.csv"), append = T, row.names = F)

    sigma0_saved = c(sigma0_saved, sigma0)

  }
  saveRDS(ll$beta, "betas.RData")
  return(list(predictive=predictive, sigma0=sigma0_saved, clusters=clusters,
              sigma2 = sigma2_post, betas = ll$beta, p = p_weights, Z = ZZ))
}


```




```{r}
nonparametric_result <- GS_nonparametric(X, Y, groups=groups, initializer_, iter=ITERS, Verbose=T, log_rate=T, sigma=0,  c0=1, d0=1)

```

#### Clusters

```{r}
### Study of clusters
p_last <- nonparametric_result$p[,dim(nonparametric_result$p)[2]]
z_last <- nonparametric_result$Z[[length(nonparametric_result$Z)]]

```

We estimate the clusters studying the posterior coclustering matrix, and then minimising the Variation of Information distance (Wade and Gahramani).

```{r}
vec2mat <- function(clust_lab){
  # in: vector clust_lab of length V s.t. clust_lab[v]=h if node v is in cluster h
  # out: binary VxH matrix M s.t. M[v,h]=1{node v is in cluster h}
  V <- length(clust_lab)
  H <- max(clust_lab)
  M <- matrix(0,V,H)
  for (v in 1:V){
    M[v,clust_lab[v]] <- 1
  }
  return(M)
}
# Compute the posterior coclustering matrix
pr_cc <- function(z_post){
  # in: posterior sample of assignments (VxN_iter matrix)
  # out: VxV matrix c with elements c[vu]=fraction of iterations in which v and u are in the same cluster
  V <- nrow(z_post)    
  N_iter <- ncol(z_post)
  c <- matrix(0,V,V)
  for (t in 1:N_iter){
    Z <- vec2mat(z_post[,t])
    c <- c + Z%*%t(Z)
  }
  return(c/N_iter)
}
```

```{r}
c_Z_DP0 <- pr_cc(t(nonparametric_result$clusters)[,BURNIN:ITERS])
memb_Z_DP_VI <- minVI(c_Z_DP0,method="avg",max.k=10)
memb_Z_DP0 <- memb_Z_DP_VI$cl
```


```{r}
data_finalclusters <- data.frame(Observations = 1:n,
                                 Residuals = residuals,
                                 Clusters = as.factor(memb_Z_DP0))
ggplot(data_finalclusters, aes(x=Observations, y=res,
                               group=Clusters)) +
  geom_point(aes(colour=Clusters)) +
  scale_colour_manual("", values=c("dodgerblue3", "seagreen4"),
                      labels = c("Cluster 1", "Cluster 2")) + ylab("Residuals") +#rtist::rtist_palette("vermeer", 2)) +
  ggtitle("Residuals & Clusters - Proposed Method")
```


# Other models of interest

Let's try other models on simulated data.

## Gaussian Process

Let's generate some data.

```{r}
X <- matrix(seq(-100, 100), by=.1, ncol=1)
n = length(X)
Y = sin(X) + rnorm(n, 0, 0.2)
```

Let's set the initialization of the hyperparameters
```{r}
n <- nrow(X)
k <- ncol(X)

b0 <-  rep(0, k+1)
B0 <-  diag(10, k+1)

s1 <-  s2 <- 2
S1 <- S2 <- 1#0.01
 
sigma2=10
theta = 1
epsilon = rnorm(n)#Y-X%*%b # ols residuals

comp <- ifelse(epsilon > 0, 1, -1)
N = 200

log_s = T
V <- rbeta(N-1,shape1 = 1,theta)
W <- cumprod(1-V)
V <- c(V,1)
W <- c(1,W)
p <- V*W

mu0 = 1
sigma0 = sqrt(25)
Z <- cbind(rtruncnorm(N,a = 0, mean = mu0, sd = sigma0), sqrt(rinvgamma(N,shape = s1, rate = S1)), sqrt(rinvgamma(N,shape = s2, rate = S2)))
complete_matrix = matrix(1, nrow=n, ncol=3)

initializer_ <- bnpResiduals:::initializer(Z, p, comp, b0, B0, N, mu0, sigma0, complete_matrix)

```

Grid for the Gaussian Process:
```{r}
x.star <- matrix(seq(-20, 20, by = 0.5), ncol=1)
```

```{r}
GPmodel <- bnp.GP(X, Y, initializer_, x.star, iter=ITERS)

```

Let's take the predictions:

```{r}
values <- cbind(x = x.star, GPmodel$preds)
prediction = apply(GPmodel$preds, 1, mean)

values <- melt(values,id="x")

df <- data.frame(cbind(x.star = x.star[,1], f.star.bar=prediction))
colnames(df) <- c("x.star", "f.star.bar")
```


```{r}
ggplot(values,aes(x=Var1,y=value)) +
  geom_line(data=data.frame(X, Y), aes(x=X, y=Y), colour="blue")+
  geom_line(data=df,aes(x=x.star,y=prediction), colour="red", size=1) + 
  geom_point(data=data.frame(x=X, y=Y),aes(x=x,y=y)) +
  theme_bw() +
  scale_y_continuous(lim=c(min(Y)-1,max(Y)+1), name="output, f(x)") +
  xlab("input, x") 
```


## AR model

Again, let's simulate some data

```{r}
n <- 100 # length of the time serie
k <- 3 # let's include k covariates

# Simulate the starting data
Y <- rnorm(n)
X <- matrix(rnorm(n*k), ncol=k)



qq = 2 # AR(qq)
# Let's compute the exact data
for (t in qq:(length(Y)-1)){
  Y[t+1] =  .5*Y[t] + .5*Y[t-1] + X[t+1,1] + X[t+1,2]+rnorm(1, 0, 5)
 }

```

We first need to define a function to get a proper design matrix:

```{r}
# Define the new design matrix, in order to be able to treat the AR model as a linear one
designmatrix.ar <- function(X, Y, q){
  k <- ncol(X)
  # Let's define the new design matrix
  Y_new <- Y[q:length(Y)]
  matrix1 <- NULL
  matrix2 <- NULL
  for (i in 0:(nrow(X)-q - 1)){
    ind1 <- sort((1+i):(i+q), decreasing = T)
    matrix1 <- rbind(matrix1, Y[ind1])
    
    if (dim(X)[1] != 0){ 
      matrix2 <- rbind(matrix2, X[q+i+1,])
      design <- cbind(matrix1, matrix2)
    }
    else{
      design <- matrix1    }
    
  }
  
  return(design)
  
}
```

Given that, we can get the X and Y to give as input to the model:

```{r}
X_design <- designmatrix.ar(X, Y, qq)
Y_design <- Y[(qq+1):length(Y)]
```

Again, we need to initialize different parameters

```{r}
# Let's use OLS to intialize residuals and parameters
b <- coef(lm(Y_design~ X_design -1))
epsilon = Y_design-X_design%*%b # ols residuals
# Initialization
b0 <-  rep(0, length(b))
B0 <-  diag(10, length(b))

s1 <-  s2 <-  2
S1 <- S2 <- 1#0.01

sigma2=10
theta = 1
comp <- ifelse(epsilon > 0, 1, -1)

N = 150
log_s = T
V <- rbeta(N-1,shape1 = 1,theta)


W <- cumprod(1-V)
V <- c(V,1)
W <- c(1,W)
p <- V*W

mu0 = 1
sigma0 = sqrt(25)

Z <- cbind(rtruncnorm(N,a = 0, mean = mu0, sd = sigma0), sqrt(rinvgamma(N,shape = s1, rate = S1)), sqrt(rinvgamma(N,shape = s2, rate = S2)))
complete_matrix = matrix(1, nrow=nrow(X_design), ncol=3)


initializer_ <- bnpResiduals:::initializer(Z = Z, p = p, comp = comp, b0 = b0, B0 = B0, 
        N = N, mu0 = mu0, sigma0 = sigma0, complete_matrix = complete_matrix)
```

The model is given by
```{r}
armodel <- bnp.ar(Y, qq, initializer_, X, iter = ITERS, burn_in = BURNIN, thin = THIN, conf_level = 0.05)

```

